{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57013f78",
   "metadata": {},
   "source": [
    "Scenario 1: Query to get who are getting equal salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a6caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Scenario-app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe53bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"001\", \"Monika\", \"Arora\", 100000, \"2014-02-20 09:00:00\", \"HR\"),(\"002\", \"Niharika\", \"Verma\", 300000, \"2014-06-11 09:00:00\", \"Admin\"),(\"003\", \"Vishal\", \"Singhal\", 300000, \"2014-02-20 09:00:00\", \"HR\"),(\"004\", \"Amitabh\", \"Singh\", 500000, \"2014-02-20 09:00:00\", \"Admin\"),(\"005\", \"Vivek\", \"Bhati\", 500000, \"2014-06-11 09:00:00\", \"Admin\")]\n",
    "\n",
    "myschema = [\"workerid\",\"firstname\",\"lastname\",\"salary\",\"joiningdate\",\"depart\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d94f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(data, myschema)\n",
    "df1.show()\n",
    "df1.groupBy(\"depart\").count().show()\n",
    "df1.groupBy(\"depart\").agg(count('*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8579e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = df1.alias(\"t1\")\n",
    "t2 = df1.alias(\"t2\")\n",
    "df_joined = t1.join(\n",
    "    t2,\n",
    "    (t1[\"workerid\"] != t2[\"workerid\"]) & (t1[\"salary\"] == t2[\"salary\"]),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "df_joined.select(\"t1.firstname\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b789f33d",
   "metadata": {},
   "source": [
    "Scenario 2: (Need the dates when the status gets changed like ordered to dispatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa3c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "      (1, \"1-Jan\", \"Ordered\"),\n",
    "      (1, \"2-Jan\", \"dispatched\"),\n",
    "      (1, \"3-Jan\", \"dispatched\"),\n",
    "      (1, \"4-Jan\", \"Shipped\"),\n",
    "      (1, \"5-Jan\", \"Shipped\"),\n",
    "      (1, \"6-Jan\", \"Delivered\"),\n",
    "      (2, \"1-Jan\", \"Ordered\"),\n",
    "      (2, \"2-Jan\", \"dispatched\"),\n",
    "      (2, \"3-Jan\", \"shipped\")]\n",
    "myschema = [\"orderid\",\"statusdate\",\"status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(data,myschema)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d483d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_result = spark.sql(\"SELECT * from orders where status='dispatched' and orderid in (select orderid from orders where status='Ordered')\")\n",
    "df2_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8b7723",
   "metadata": {},
   "source": [
    "Scenario 3: difference from next order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab79378",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1111, \"2021-01-15\", 10),\n",
    "        (1111, \"2021-01-16\", 15),\n",
    "        (1111, \"2021-01-17\", 30),\n",
    "        (1112, \"2021-01-15\", 10),\n",
    "        (1112, \"2021-01-15\", 20),\n",
    "        (1112, \"2021-01-15\", 30)]\n",
    "\n",
    "myschema = [\"sensorid\", \"timestamp\", \"values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.createDataFrame(data, myschema)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ee1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "wspec = Window.partitionBy(\"sensorid\").orderBy(\"values\")\n",
    "\n",
    "final_df = df3.withColumn('newValue', lead(\"values\", 1).over(wspec)) \\\n",
    "            .filter(col('newValue').isNotNull()) \\\n",
    "            .withColumn('values', expr(\"newValue - values\")) \\\n",
    "            .drop(col('newValue')) \\\n",
    "            .orderBy('sensorid')\n",
    "\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c373c2",
   "metadata": {},
   "source": [
    "scenario 4\n",
    "user third transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dda719",
   "metadata": {},
   "source": [
    "user_id\t    spend\ttransaction_date\n",
    "111\t        100.50\t01/08/2022 12:00:00\n",
    "111\t        55.00\t01/10/2022 12:00:00\n",
    "121\t        36.00\t01/18/2022 12:00:00\n",
    "145\t        24.99\t01/26/2022 12:00:00\n",
    "111\t        89.60\t02/05/2022 12:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6889a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data = [('111','100.50','01/08/2022 12:00:00'),('111','55.00','01/10/2022 12:00:00'),('121','36.00','01/18/2022 12:00:00'),('145','24.99','01/26/2022 12:00:00'),('111','89.60','02/05/2022 12:00:00')]\n",
    "columns = ['user_id','spend','transaction_date']\n",
    "\n",
    "df4 = spark.createDataFrame(data, columns)\n",
    "\n",
    "df4 = df4.withColumn(\"spend\", col(\"spend\").cast(\"float\")) \\\n",
    "        .withColumn(\"transaction_date\", to_date(col(\"transaction_date\"), \"MM/dd/yyyy HH:mm:ss\"))\n",
    "\n",
    "w_spec = Window.partitionBy(col('user_id')).orderBy(col('transaction_date').desc())\n",
    "\n",
    "df4 = df4.withColumn(\"transaction_no\", row_number().over(w_spec)).filter(col('transaction_no')==3)\n",
    "\n",
    "df4.printSchema()\n",
    "df4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"fillna_example\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Alice\", None, 30),\n",
    "    (\"Bob\", 25, None),\n",
    "    (None, 35, 40),\n",
    "    (\"Charlie\", 40, 50)\n",
    "]\n",
    "columns = [\"Name\", \"Age\", \"Score\"]\n",
    "df5 = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Fill 'Name' with 'Unknown', 'Age' with 0, and 'Score' with the mean score\n",
    "mean_score = df5.select(\"Score\").agg({\"Score\": \"avg\"}).collect()[0][0]\n",
    "print(mean_score)\n",
    "df_filled_different = df5.fillna({\"Name\": \"Unknown\", \"Age\": 0, \"Score\": mean_score})\n",
    "df_filled_different.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ee65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [(\"John,Doe\",), (\"Jane,Smith\",), (\"Peter,Jones\",), (\"Alice,Schultz\",)]\n",
    "df6 = spark.createDataFrame(data, [\"FullName\"])\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df6 = df6.withColumn('firstName', split(col('FullName'), \",\")[0]) \\\n",
    "       .withColumn('lastName', split(col('FullName'), \",\")[1])\n",
    "\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf16c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,\"John,Doe\",), (1,\"Jane,Smith\",), (2,\"Peter,Jones\",), (2,\"Alice,Schultz\",)]\n",
    "df = spark.createDataFrame(data, [\"gid\", \"FullName\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545da29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = df.select(\"gid\", explode(split(col(\"FullName\"),\",\")).alias(\"splt\"))\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4eb632",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = result_df.groupBy(\"gid\").agg(collect_list(col(\"splt\")).alias(\"cl\"))\n",
    "result_new_df = new_df.select(\"gid\", concat_ws(\",\", col(\"cl\")))\n",
    "result_new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6be947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, lead, col, when, count, lit, max as max_\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Example data\n",
    "data = [\n",
    "    (1, \"apple\"),\n",
    "    (2, \"banana\"),\n",
    "    (3, \"carrot\"),\n",
    "    (4, \"dates\"),\n",
    "    (5, \"eggfruit\")\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"order_id\", \"item\"])\n",
    "\n",
    "# Step 1️⃣ Create window spec\n",
    "wspec = Window.orderBy(\"order_id\")\n",
    "\n",
    "# Step 2️⃣ Add lag and lead columns (equivalent to CTE_1)\n",
    "cte_1 = df.withColumn(\"prev_item\", lag(\"item\", 1).over(wspec)) \\\n",
    "          .withColumn(\"next_item\", lead(\"item\", 1).over(wspec))\n",
    "\n",
    "# Step 3️⃣ Find max order_id (equivalent to SELECT COUNT(*) in SQL)\n",
    "max_order = cte_1.agg(max_(\"order_id\").alias(\"max_order_id\")).collect()[0][\"max_order_id\"]\n",
    "\n",
    "# Step 4️⃣ Apply CASE WHEN logic\n",
    "final_df = cte_1.withColumn(\n",
    "    \"final_item\",\n",
    "    when(col(\"order_id\") % 2 == 0, col(\"next_item\"))\n",
    "    .when((col(\"order_id\") % 2 != 0) & (col(\"order_id\") == lit(max_order)), col(\"item\"))\n",
    "    .otherwise(col(\"prev_item\"))\n",
    ")\n",
    "\n",
    "final_df.select(\"order_id\", \"final_item\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
