{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e47c11",
   "metadata": {},
   "source": [
    "json_sample=\"\"\"{\n",
    " \"id\": \"0001\",\n",
    " \"type\": \"donut\",\n",
    " \"name\": \"Cake\",\n",
    " \"ppu\": 0.55,\n",
    " \"batters\":\n",
    "  {\n",
    "   \"batter\":\n",
    "    [\n",
    "     { \"id\": \"1001\", \"type\": \"Regular\" },\n",
    "     { \"id\": \"1002\", \"type\": \"Chocolate\" },\n",
    "     { \"id\": \"1003\", \"type\": \"Blueberry\" },\n",
    "     { \"id\": \"1004\", \"type\": \"Devil's Food\" }\n",
    "    ]\n",
    "  },\n",
    " \"topping\":\n",
    "  [\n",
    "   { \"id\": \"5001\", \"type\": \"None\" },\n",
    "   { \"id\": \"5002\", \"type\": \"Glazed\" },\n",
    "   { \"id\": \"5005\", \"type\": \"Sugar\" },\n",
    "   { \"id\": \"5007\", \"type\": \"Powdered Sugar\" },\n",
    "   { \"id\": \"5006\", \"type\": \"Chocolate with Sprinkles\" },\n",
    "   { \"id\": \"5003\", \"type\": \"Chocolate\" },\n",
    "   { \"id\": \"5004\", \"type\": \"Maple\" }\n",
    "  ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb44a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Flatten_json\").getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"multiline\", True).json(\"/Users/hasan/Nasrul/Learning/Python/Data-Engineering/de-tutorials/data/sample.json\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9488ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"new_batters\", explode(\"batters.batter\")) \\\n",
    "    .withColumn(\"new_toppings\", explode(\"topping\")).drop(\"batters\", \"topping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af948e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"id\", \"name\", \"ppu\", \"type\", \n",
    "          col(\"new_batters.id\").alias(\"new_batters_id\"),\n",
    "          col(\"new_batters.type\").alias(\"new_batters_type\"),\n",
    "          col(\"new_toppings.id\").alias(\"new_toppings_id\"),\n",
    "          col(\"new_toppings.type\").alias(\"new_toppings_type\")\n",
    "        \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2271f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6478aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json(\"/Users/hasan/Nasrul/Learning/Python/Data Engineering/de-pyspark-interview/data/flattened_sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a505c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = spark.read.json(\"/Users/hasan/Nasrul/Learning/Python/Data Engineering/de-pyspark-interview/data/flattened_sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7666f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-pyspark-interview (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
