{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7f57347",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸš€ Day 1: PySpark Fundamentals & Hands-On Prep\n",
    "\n",
    "Welcome to Day 1 of PySpark Interview Prep!  \n",
    "This notebook focuses on **DataFrame basics**, **transformations**, and **actions** â€” the foundation of Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a19b1",
   "metadata": {},
   "source": [
    "## âœ… Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab911ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, avg\n",
    "spark = SparkSession.builder.appName(\"Day1-InterviewPrep\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905f4fe",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Create DataFrame from List\n",
    "\n",
    "**Q1.** Create a PySpark DataFrame with the following data:\n",
    "\n",
    "| Name   | Age | City           |\n",
    "|--------|-----|----------------|\n",
    "| Alice  | 29  | New York       |\n",
    "| Bob    | 35  | San Francisco  |\n",
    "| Cathy  | 25  | Seattle        |\n",
    "\n",
    "- Show the DataFrame\n",
    "- Print its schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c18d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df = spark.createDataFrame([('Alice', 29, 'New York'), ('Bob', 35, 'San Francisco'), ('Cathy', 25, 'Seattle')],[\"Name\", \"Age\", \"City\"])\n",
    "\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3624b72",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Filter Rows by Age\n",
    "\n",
    "**Q2.** Filter and return only rows where Age > 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df_greater_than_30 = df.filter(col(\"Age\")>30)\n",
    "df_greater_than_30.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17f152",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Add Column Based on Condition\n",
    "\n",
    "**Q3.** Add a new column `IsAdult` where:\n",
    "- True if Age >= 18\n",
    "- False otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb960c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df = df.withColumn(\"IsAdult\", when(df.Age>=18, True).otherwise(False))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db217696",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Group By and Aggregate\n",
    "\n",
    "**Q4.** Using the following data, find average salary per department:\n",
    "\n",
    "| Department | Employee | Salary |\n",
    "|------------|----------|--------|\n",
    "| HR         | John     | 4000   |\n",
    "| HR         | Alice    | 3500   |\n",
    "| IT         | Bob      | 4500   |\n",
    "| IT         | Cathy    | 4800   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e96556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "company_df = spark.createDataFrame([(\"HR\", \"John\", 4000),(\"HR\", \"Alice\", 3500),(\"IT\", \"Bob\", 4500),(\"IT\", \"Cathy\", 4800)],[\"Department\", \"Employee\", \"Salary\"])\n",
    "\n",
    "company_df.groupBy(\"Department\").agg(avg(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75353c68",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Read and Write Files\n",
    "\n",
    "**Q5.** Read a CSV file and write it back as a Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "emp_df = spark.read.format(\"csv\").option(\"header\", True).option(\"inferschema\", True).load(\"./data/employee.csv\")\n",
    "\n",
    "emp_df.show()\n",
    "\n",
    "emp_df.write.parquet(\"./data/employee.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123d852a",
   "metadata": {},
   "source": [
    "## ðŸ” Section 2: Column Operations & Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2caf967",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Add Column: Name Length\n",
    "\n",
    "**Q6.** Add a new column `NameLength` which stores the length of each name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b2c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from pyspark.sql.functions import length\n",
    "df = df.withColumn(\"NameLength\", length(df.Name))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3543b630",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Add Column: Short City Code\n",
    "\n",
    "**Q7.** Add a column `CityCode` with the first 3 letters of the city in uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80efcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from pyspark.sql.functions import substring, upper\n",
    "df = df.withColumn(\"CityCode\", lit(upper(substring(df.City, 1, 3))))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9dd2ce",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Use Regex & String Filters\n",
    "\n",
    "**Q8.** Filter rows where City starts with 'S'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a99963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df.filter(df.City.like('S%')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09e8cba",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Drop Column\n",
    "\n",
    "**Q9.** Drop the column `City`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94610586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df = df.drop(col(\"City\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034c38b",
   "metadata": {},
   "source": [
    "## ðŸ”„ Section 3: Sorting, Deduplication, Limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b3c5ad",
   "metadata": {},
   "source": [
    "## ðŸ”Ÿ Sort by Age (Descending)\n",
    "\n",
    "**Q10.** Sort the DataFrame by Age descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182721b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df.orderBy(col(\"Age\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4ac23",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£1ï¸âƒ£ Remove Duplicates\n",
    "\n",
    "**Q11.** Drop duplicate records based on the Name column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df = df.dropDuplicates([\"Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a73e1d",
   "metadata": {},
   "source": [
    "## ðŸ§¼ Section 4: Null Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7fee6",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£2ï¸âƒ£ Filter Null or Empty Department\n",
    "\n",
    "**Q12.** Filter out rows where Department is null or empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126812c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "emp_df.filter(col(\"Department\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a062c1d",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£3ï¸âƒ£ Fill Missing Values\n",
    "\n",
    "**Q13.** Fill missing Department with `\"Unknown\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab62c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "emp_df.fillna(\"Unknown\", (\"Department\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c2df3",
   "metadata": {},
   "source": [
    "## ðŸ§ª Section 5: Type Casting & Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aba280",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£4ï¸âƒ£ Cast Salary from String to Integer\n",
    "\n",
    "**Q14.** Create a DataFrame with Salary as string, then cast it to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec23508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "tst_df = spark.createDataFrame([(\"max\", \"2300\"),(\"tom\", \"3100\")],[\"Name\", \"Salary\"])\n",
    "tst_df.printSchema()\n",
    "\n",
    "# convert salary to int\n",
    "casted_tst_df = tst_df.select(col(\"Name\"), col(\"Salary\").cast(IntegerType()))\n",
    "casted_tst_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514f863",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£5ï¸âƒ£ Print Schema & Column Types\n",
    "\n",
    "**Q15.** Use `.printSchema()` and `df.dtypes` to inspect types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1fa233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df.printSchema()\n",
    "emp_df.printSchema()\n",
    "casted_tst_df.printSchema()\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0791903b",
   "metadata": {},
   "source": [
    "## âš™ï¸ Section 6: Transformations vs Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac13c6",
   "metadata": {},
   "source": [
    "## ðŸ§  Q16: What are Transformations and Actions?\n",
    "\n",
    "ðŸ“ Spark separates operations into two types:\n",
    "- **Transformations**: Lazy (e.g., `filter`, `select`, `groupBy`)\n",
    "- **Actions**: Trigger computation (e.g., `show`, `collect`, `write`)\n",
    "\n",
    "> Q: Why does nothing happen when you run `filter()` without `.show()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19418532",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Write your explanation here\n",
    "```\n",
    "Transformations are lazy operations that define a new DataFrame (e.g. select, filter, withColumn). They don't trigger execution.\n",
    "\n",
    "Actions are operations that trigger a Spark job and result in execution (e.g. show(), collect(), write()).\n",
    "\n",
    "Example:\n",
    "df.filter(col(\"age\") > 30) does nothing until you call df.show() or df.write()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c31b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "454fdf85",
   "metadata": {},
   "source": [
    "## ðŸ§  Q17: What is Lazy Evaluation?\n",
    "\n",
    "> Q: Explain lazy evaluation in Spark and why it's beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7428f0",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Write your explanation here\n",
    "```\n",
    "Spark builds a logical execution plan and waits to execute until an action is called.\n",
    "\n",
    "This allows Spark to optimize the entire job using techniques like pipelining, predicate pushdown, and logical plan pruning.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "1. Efficient execution\n",
    "2. Avoids unnecessary computation\n",
    "3. Enables automatic optimizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ed64f",
   "metadata": {},
   "source": [
    "## ðŸ§  Q18: What are Wide vs Narrow Transformations?\n",
    "\n",
    "> Q: Give an example of each and explain their difference in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d93a63",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Write your explanation here\n",
    "```\n",
    "Narrow: No data shuffle, each partition is read independently\n",
    "Examples: select, filter, map, withColumn\n",
    "\n",
    "Wide: Requires shuffle across partitions (network heavy)\n",
    "Examples: groupBy, join, distinct, repartition\n",
    "\n",
    "Wide transformations can slow down jobs and require careful planning with partitioning and caching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b23d55e",
   "metadata": {},
   "source": [
    "## ðŸ§  Q19: How Does Spark Read a CSV Differently Than Parquet?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4a103a",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Write your explanation here\n",
    "```\n",
    "#### CSV vs Parquet in Spark\n",
    "\n",
    "| Feature        | CSV                          | Parquet                       |\n",
    "|----------------|------------------------------|-------------------------------|\n",
    "| Format         | Text-based                   | Binary, columnar              |\n",
    "| Performance    | Slower (no compression/index)| Faster (compressed & indexed) |\n",
    "| Schema         | Inferred or manually defined | Embedded within file          |\n",
    "| Compression    | None or external             | Built-in (Snappy by default)  |\n",
    "| Readability    | Human-readable               | Machine-efficient             |\n",
    "| Use Case       | Simple ingestion, logs       | Analytics, reporting, large datasets |\n",
    "\n",
    "> ðŸ’¡ Use **Parquet** for optimized performance when dealing with large, structured data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186693ed",
   "metadata": {},
   "source": [
    "## ðŸ§  Q20: Explain Repartition vs Coalesce\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612961b6",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Write your explanation here\n",
    "```\n",
    "#### `repartition()` vs `coalesce()` in Spark\n",
    "\n",
    "| Feature          | `repartition()`                          | `coalesce()`                        |\n",
    "|------------------|-------------------------------------------|-------------------------------------|\n",
    "| Purpose          | Increase or redistribute partitions       | Reduce number of partitions         |\n",
    "| Shuffle          | Yes (full shuffle of data)                | No (narrow transformation)          |\n",
    "| Cost             | Expensive due to shuffling                | Efficient, avoids full shuffle      |\n",
    "| Use Case         | After filtering or joins to rebalance     | Before writing to avoid small files |\n",
    "| API Example      | `df.repartition(10)`                      | `df.coalesce(1)`                    |\n",
    "\n",
    "> âœ… Use `repartition()` to increase partitions and balance load.  \n",
    "> âœ… Use `coalesce()` to reduce output files efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7731456",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸŽ¯ Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "- Core DataFrame operations\n",
    "- Aggregations and file IO\n",
    "- Schema awareness\n",
    "- Column and null handling\n",
    "- Essential theory behind Sparkâ€™s execution\n",
    "\n",
    "ðŸ“Œ Ready for Day 2? We'll dive into **joins**, **transformations**, and **interview-worthy questions** on data movement and optimization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
