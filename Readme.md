# ðŸš€ PySpark Interview Prep â€“ 5 Day Intensive Bootcamp

Welcome to the **5-Day PySpark Interview Prep Bootcamp**, designed for data engineers and analysts preparing for real-world interviews. This plan helps you build deep understanding and hands-on fluency with PySpark â€” from basic DataFrame operations to performance tuning and distributed computing concepts.

---

## ðŸŽ¯ Who This Is For

- Data engineers with **1â€“4 years of experience**
- SQL users transitioning into Spark
- Professionals preparing for **data engineering interviews**
- Anyone who wants to solidify their **PySpark fundamentals and intermediate skills**

---

## ðŸ“… 5-Day Learning Plan

| Day | Focus Area                                      | Topics                                                       |
|-----|--------------------------------------------------|---------------------------------------------------------------|
| 1   | PySpark Basics & DataFrames                     | DataFrame creation, filters, aggregations, file IO, schema   |
| 2   | Joins, Transformations, String Ops              | All join types, select, drop, `withColumn`, regex, alias     |
| 3   | Aggregations, Grouping, Nulls, Data Types       | Multi-agg, grouping, null handling, casting, date functions  |
| 4   | Optimizations, Partitioning, Broadcast Joins    | `repartition`, `coalesce`, shuffle, broadcast joins, formats |
| 5   | SQL, Window Functions, Theory, Final Challenges | Spark SQL, windowing, job internals, tuning, full project    |

---

## ðŸ§  Interview Topics & Theory Included

Each notebook includes:

- Short, **interview-focused theory**
- Hands-on **code problems**
- Real-world usage patterns
- Questions you may face in interviews

Example Theory Topics Covered:
- **Lazy evaluation** in Spark
- **Transformations vs actions**
- **Wide vs narrow transformations**
- **Partitioning strategies**
- **Broadcast joins vs shuffle joins**
- **Caching vs persistence**
- **DAG and job execution flow**
- **Data formats: CSV vs Parquet vs ORC**
- **Tungsten and Catalyst optimizers**
- **How to choose number of executors and cores**
- **Skewed joins and mitigation strategies**
- **Repartition vs Coalesce â€” which and when**
- **Spark SQL vs DataFrame API**
- **Handling large joins with broadcast hint**
- **When and why to use window functions**

---

## ðŸ“˜ Sample Interview Questions Covered

- How do you optimize a Spark job reading 1 TB of Parquet data?
- What is the difference between repartition and coalesce?
- When should you broadcast a table in Spark?
- What causes a shuffle in Spark and how do you reduce it?
- How does Spark handle schema inference?
- Whatâ€™s the difference between `select` and `withColumn`?
- How do you handle null values in a production ETL job?
- Explain Sparkâ€™s physical execution model (job â†’ stage â†’ task).
- What are narrow and wide transformations?
- How do you determine number of executors and memory in a job?

---


---

## ðŸ›  How to Use This Repo

1. Clone the repo locally.
2. Follow each day's notebook step-by-step.
3. Solve the code problems *before* looking at the solutions.
4. Review theory notes regularly â€” especially before interviews.
5. Extend each notebook with your own questions or variations.
6. Revisit Day 5 before the actual interview â€” it's a packed revision set.

---

## ðŸ“ˆ Tools Required

- Python 3.x
- PySpark (>= 3.0)
- Jupyter Notebook or VSCode
- Git (for pushing updates and tracking changes)

---

## ðŸ’¡ Want to Contribute?

Open a pull request with:
- Additional interview questions
- Dataset links
- Real-world code snippets
- Bug fixes or better explanations

---

## ðŸ”— License

MIT License â€” Free to use, improve, and share.

---

**Letâ€™s build PySpark confidence â€” one notebook at a time.**
